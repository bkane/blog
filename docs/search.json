[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am blogging about trying to learn about machine learning because they told me to."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ben’s Machine Learning Blog",
    "section": "",
    "text": "From Scratch Pt II\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nFrom Scratch\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nHello Machine Learning World\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nBen Kane\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello Machine Learning World",
    "section": "",
    "text": "This is the first post in a Quarto blog. Turns out it’s Quarto, not Quatro. I only discovered that a few minutes ago.\nThen again, I didn’t know what Quatro was at all half an hour ago. I was looking at fastpages, which is a way of writing blog posts on GitHub Pages and incorporating Jupyter notebook cells and output. And what is a Jupyter notebook? Well it’s like an environment where you can mix Python code and markdown text for a very interactive way of experimenting with and demonstrating machine concepts. And you can run these notebooks in the cloud in places like Colab or Kaggle. Then you can export your notebook code to regular Python and make an app.py file that creates a Gradio interface for running a machine learning model on a HuggingFace Space like this Bear Classifier I made.\nWhat the heck did I just say?\nI’ve used Python off and on for about 15 years now. It’s been a language I dabble with for a week or two to try out something like writing a Discord bot, generating web pages from a template, doing some web scraping, or messing about with machine learning/deep learning. I play for a bit, get a bit bogged down with package management and environment management, learn about some new templating language, a new meta-meta-meta install manager with some cute name like anaconda or conda or cda or whatever. Then after my experiment runs its course, I promptly drop Python and go back to whatever it is I’m actually doing.\nIt’s a very similar to the experience I have with web development, where there are myriad packages and frameworks and versions of everything, all packed with syntactic sugar and code generation ostensibly to make it easy to pick up. But the dependencies always collapse, the documentations and the tutorials always get outdated, and you’re left trying to follow the trail from breadcrumb to breadcrumb in order to get something working, usually without really understanding how it all comes together. And it’s not for lack of trying! You can dig in and see how the tools and templates and managers all work, but ultimately it just feels like layers of frameworks obfuscating the task at hand.\nOf course, when the layers of ‘simplifying’ frameworks get to be too much… you need to make a framework that eschews all the cruft and really makes things easy.\n\n\n\nStandards\n\n\n\nLearning So Far\nAnyway, here I am writing a blog to record how I’m feeling along this journey of learning machine learning (for probably the second or third time). I’ve been following the fastai course which has been great for getting things off the ground quickly. It also recommended I start blogging, so here we are.\nSo far, I’ve done two video lessons and the two accompanying chapters of the book (which take slightly different approaches to the same concepts). I’m appreciating the reinforcement thus far, because I’ve bounced off this subject in the past.\nSome big, immediate learnings: you can use a small amount of data to fine-tune an existing model, and get something useful for a standard application to just call like any other function. The idea of integrating a trained model into something like a video game is actually incredibly… doable. Sure, there are frameworks and languages and whatever other hurdles to overcome before I just drop a method into a Unity game, but it’s really not that far off.\nIn the first couple of lessons, I’ve fine-tuned an image classifer based on the resnet18 model. We’ve even dabbled with the HuggingFace community site to make a public Space, which is a site that hosts the model I trained to classify images between black bears, grizzly bears, and teddy bears. It’s not very sophisticated, but it’s a surprisingly complete end-to-end deployment of a thing that uses an image classification model. And you can just use it right now!\n\n\n\nA basic bear classifier that I made ➡ Click to try it out on huggingface.co!\n\n\nOkay, one more thing before I go: this blog post can have Python code in it, like a notebook. I should try that out, otherwise this has just been a really, really complicated Wordpress site.\n\n\nCode\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nurls = search_images('grizzly bear', max_images=1)\ndest = 'grizzly.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\nSearching for 'grizzly bear'\n\n\n\n\n\nNow to get this blog post rendered and committed to a GitHub repo and published to GitHub Pages, preferably using an automated GitHub Action……….\nThis was a lot easier in the GeoCities days."
  },
  {
    "objectID": "posts/2023-05-03 from scratch/index.html",
    "href": "posts/2023-05-03 from scratch/index.html",
    "title": "From Scratch",
    "section": "",
    "text": "Aha, bet you didn’t think I’d write a second post! Wait, that was me."
  },
  {
    "objectID": "posts/2023-05-03-from-scratch/index.html",
    "href": "posts/2023-05-03-from-scratch/index.html",
    "title": "From Scratch",
    "section": "",
    "text": "Aha, bet you didn’t think I’d write a second post! Wait, that was me.\nIt’s been just over a week since my first post, which I wrote after completing Chapter 2 of the fast.ai “Practical Deep Learning for Coders” course. The course takes a sort of “top down” approach to the topic and has you creating an image classifer right off the bat, followed by deploying another one to a production website by the end of the second chapter. It’s fast! It’s satisfying! It’s… a bit magic.\nSo Chapter 3 brings you back down to Earth and starts explaining how this all works. This is more of a traditional groundwork-laying chapter and it’s where I’ve dabbled with this stuff in the past (and quickly dropped off from). The second time around though? Stuff is starting to stick.\nThe fastai course has both videos and a ‘book’, which is really a series of Jupyter notebooks (though you can buy a print copy if you want - I imagine it’s much less effective to be honest). They sort of cover mostly the same topics but they do handle it in different ways, and this reinforcement is working well for me. It feels slow (because you’re exploring the same ground twice) but this is a pretty fundamental and meaty chapter, so I appreciate it here.\nIn fact, by the end of the chapter, simple neural networks in PyTorch, with the exception of gradient calculations, have been pretty well demystified. I’m still not comfortable with all this stuff yet but I was able to follow along quite well.\n(Alright, I just spent 15 minutes trying to get some ipywidgets to work in this blog post but although the sliders I added worked, they didn’t result in a graph getting re-plotted. Oh well, moving on.)\nSo what have I really learned so far?\n\nA Quadratic Example\nGiven some input to a function, you want to get some ouput. To start, let’s consider a quadratic function. You collect some data from the real world, and it looks like this:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom numpy.random import normal,seed,uniform\nfrom functools import partial\nfrom fastbook import *\n\nnp.random.seed(42)\n\ndef quad(a,b,c,x): return a*x**2 + b*x + c\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\n\nf = mk_quad(3, 2, 1)\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.3, 1.5)\nplt.scatter(x,y)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1c0120ba050&gt;\n\n\n\n\n\nYou’d like to have a model (a function here) that could take some input value x and output a good prediction for y. It looks like a quadratic function would work well (because we contrived this example from one), so let’s find some values for \\(ax^2 + bx + c\\).\nWe can start with random values for a, b, and c. Then we need to figure out how good they are. In this case, we’d create a function to calculate how bad they are: a mean squared error loss function. Pick some parameter values, calculate the loss. Pick some new parameters, calculate the loss again, see if it went up or down.\n\ndef mse(preds, targets):\n  return ((preds-targets)**2).mean()\n\nNext step would be to automate it. Rather than guess and check though, we can calculate the gradient for the input values. That is, how will each change in the parameter affect the resulting loss? That way, we know which way to change a parameter and, roughly, how big of an effect it will have. PyTorch can calculate these gradients for us, by tagging the input tensor as requires_grad. That means that when the tensor is used in a calculation, PyTorch will perform some internal magic and calculate the gradient for it.\n\ndef quad_mse(params):\n  f = mk_quad(*params) # \"*\" is a Python thing that means spread \n                       # the array across the function arguments here\n  return mse(f(x), y)\n\nquad_mse([1.5, 1.5, 1.5])\n\ntensor(5.8336, dtype=torch.float64)\n\n\nHere’s how we create an input tensor for our parameters, flag it to say “please calculate the gradients”, and then do so. Note that we called backward() (for back propagation) on the resulting tensor, not the input tensor. This is because we want to figure out how a change in the input parameters will affect the output. The values of the gradients, however, live on the input tensor. It does make sense, but it also feels a bit back and forth-y.\n\n# input tensor\nabc = torch.tensor([1.5,1.5,1.5], requires_grad=True)\nabc\n\ntensor([1.5000, 1.5000, 1.5000], requires_grad=True)\n\n\n\n# output tensor\nloss = quad_mse(abc)\nloss\n\ntensor(5.8336, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nThe grad_fn=&lt;MeanBackward0&gt; shows us that PyTorch can calculate gradients from this tensor. Then we call backwards() to do it, and check the resulting grad values on the input tensor:\n\nloss.backward()\nabc.grad\n\ntensor([-5.1419,  2.8472, -1.1009])\n\n\nThis tells us that increasing the value of the first parameter will decrease the loss a fair bit, decreasing the value of the second parameter will decrease the loss a little bit, and increasing the last parameter will decrease the loss a very little bit.\nSo now take a step in that direction by modifying our input parameters and calculating the loss again. Note that since modifying the input parameter is “using the tensor in a calculation”, we need to tell PyTorch that this shouldn’t be used to update the gradients on it (or things get wonky).\n\nwith torch.no_grad():\n  abc -= abc.grad*0.01\n  loss = quad_mse(abc)\n\nprint(f'loss={loss:.2f}')\nprint(abc)\n\nloss=5.49\ntensor([1.5514, 1.4715, 1.5110], requires_grad=True)\n\n\nThe loss went from 5.8336 to 5.49, and we can see each of the parameters moved in the direction to minimize the loss! Hooray. Note that we modified abc by moving in the opposite direction of the gradient (to minimize loss, not maximize it) and we also only moved by 0.01, which is called the learning rate. Moving parameters in the opposite direction of the gradient is called gradient descent.\nIf the learning rate is too low, it will take a long (too long) time to get meaningful results. If it’s too big, the modifications to parameters will be so large that you won’t converge on a useful solution but instead bounce around getting worse results.\nOkay so now you could run this a bunch of times and get closer to an ideal set of values for a, b, and c that fit the data well. Cool, now we know how to automatically find parameters to fit a function.\n\n\nBut what about problems that aren’t modeled by quadratics?\nFinding great parameters for quadratics isn’t super useful, since they don’t inherently model a lot of things. However, what if I told you there was a magical formula that could model any problem? Well more specifically, a function that could solve any computable problem to an arbitrariliy high level of accuracy?\nFor any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together. To make it closer to the wiggly function, we just have to use shorter lines. This is known as the universal approximation theorem.\nTo start, let’s actually downgrade our quadratic example to a linear equation: \\(y = mx + b\\)\nNow if you add together a bunch of linear equations, you’re just going to get a different linear equation, which is still going to be a straight line. In order to get our “short line segment” property to follow an arbitrarily wiggly line, we need to be able to have these equations affect different parts of the output. Enter the rectified linear equation:\n\ndef rectified_linear(m,b,x):\n  y = m*x + b\n  return torch.clip(y, 0.)\n\nplot_function(partial(rectified_linear, 1,1))\n\n\n\n\nA rectified linear equation is just a linear equation where any value &lt; 0 is set to 0. That’s it.\nNow, if you combine two of these together, you can see how you could start to define arbitrary shapes:\n\ndef double_relu(m1,b1,m2,b2,x):\n  return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\nplot_function(partial(double_relu, 1, 1, -2, -1))\n\n\n\n\nNow if we add together as many of these rectified linear units (aka ReLUs), we could match any arbitrarily wiggly function to whatever degree of accuracy we desire. And it works in more dimensions too (i.e. functions with more than one input).\nOkay but where do m1,b1,m2,b2, etc all come from to match the wiggly function? Well that’s the gradient descent iterative approach from earlier.\nIn a very smoothed over and blurry nutshell: this is machine learning.\nPhew this is a long post! This actually only covers the video portion of the chapter. I’ll do a recap of the book part later."
  },
  {
    "objectID": "posts/post-next/index.html",
    "href": "posts/post-next/index.html",
    "title": "From Scratch Pt II",
    "section": "",
    "text": "I’m back, covering the second part of chapter 3: the book part. Which is roughly chapter 4 of the book. Getting “Don’t Do What Donny Don’t Does” vibes with this every so often.\nAnyway, this chapter worked with part of the MNIST dataset, which is a bunch of small 28x28 images of hand-written digits. It’s just a partial dataset, looking at 3s and 7s, and trying to distinguish them.\nWe started by coming up with a non-machine learning baseline: stacking all the images of the 3s and averaging them to get a fuzzy “ideal” 3. Repeat for the 7s. Then for any test image, we could compare how closely it matched each of the ‘ideal’ digits. In theory, any given 3 would have more in common with the fuzzy averaged 3 than the fuzzy average 7.\nHere we learned about how to quantify things like ‘how closely an image matches another’ by using a loss function such as the L1 norm. Turns out that’s just the mean absolute difference: (test3 -avg3).abs().mean() There’s also the root mean squared error, where you square the difference, take the mean, then take the square root: ((test3 - avg3)**2).mean().sqrt()\nBoth are just ways of ensuring errors in either direction (positive and negative) are able to accumulate rather than just cancelling out.\nTurns out this approach isn’t bad! But the test case is very basic: 3s and 7s are pretty different after all."
  },
  {
    "objectID": "posts/post-next/index.html#an-end-to-end-example-of-stochastic-gradient-descent",
    "href": "posts/post-next/index.html#an-end-to-end-example-of-stochastic-gradient-descent",
    "title": "From Scratch Pt II",
    "section": "An end to end example of Stochastic Gradient Descent",
    "text": "An end to end example of Stochastic Gradient Descent\nThis was great. It was the parabola example from the video, but I implemented it all the way through for this 7 step machine learning process:\n\n\n\nThis is how machine learning works it would seem\n\n\n\nInitialize the parameters\nCalculate the predictions\nCalculate the loss\nCalculate the gradients\nStep the weights\nRepeat the process\nStop\n\n“Initialize the parameters” is the stochastic part: i.e. choose random values. “Calculate the predictions” means run the quadratic function with our current parameter values. Here we also plotted results, which was great visualization. “Calculate the loss” was done by writing our own mean squared error function (see above). “Calculate the gradients” was the loss.backward() call to get the params.grad value. “Step the weights” is the gradient descent part of the name (subtract a small fraction [learning rate] of the gradient from the params) “Repeat the process” and “Stop” were some loops.\nThis part was great! It was a solid reinforcement of the same concept shown in the video."
  },
  {
    "objectID": "posts/post-next/index.html#apply-this-to-the-mnist-loss-function",
    "href": "posts/post-next/index.html#apply-this-to-the-mnist-loss-function",
    "title": "From Scratch Pt II",
    "section": "Apply this to the MNIST Loss Function",
    "text": "Apply this to the MNIST Loss Function\nNext we did the same process but with the MNIST data set. Immediately we run into some more practical considerations regarding the function we have to use and the dimensions of the problem.\nWe know we’re going to model it with a linear equation (\\(y = mx + b\\)). But the input isn’t just a single \\(x\\), it’s a 28x28 image. So really it’s \\(y = m_1x_1 + m_2x_2 + m_3x_3 + ... + m_{784}x_{784} + b\\). I think. We end up with a tensor of size 784 to hold the weights (parameters) for our model.\nHere we start learning about performance optimizations already. Doing a repeated sum of products as described above is just matrix multiplication, which is def linear1(xb): xb@weights + bias.\nAnother important concept is the difference between our metric and the loss function. Basically, the metric is something we use to drive human understanding whereas the loss function is something we use to drive the machine learning process.\nThe difference: our metric here is accuracy, which is a binary right or wrong for every image in the test set. It could take a big change in the behaviour of the model to flip a prediction from wrong to right (or vice versa), so it wouldn’t be good for taking tiny learning steps. Conversely, the loss function conveys how close (or far) a prediction is from being correct and can be used to tune a model. It’s not as immediately intuitive for human consumption though (although arguably people may care about how ‘confident’ a prediction is).\nThere’s more work getting this example through the process, a bunch of which is bogged down somewhat by wrangling tensor dimensions. As we go through each part though, we find (unsurprisingly!) that PyTorch and/or fastai have built-in definitions for common functions like sigmoid() (for ensuring data is in the range \\((0, 1)\\)), creating linear models and initializing parameters (nn.Linear), a stochastic gradient descent optimizater (SGD(model, learning_rate)), and a learning process loop (Learner, which takes a DataLoaders object, which itself is a DataLoader for both the training and validation sets). It’s all good stuff.\nBasically we write it all from scratch to understand what something like this is actually doing:\n\nlearn = Learner(DataLoaders(dl, valid_dl), nn.Linear(28*28), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)\nlearn.fit(10, lr=1)"
  },
  {
    "objectID": "posts/post-next/index.html#non-linearity",
    "href": "posts/post-next/index.html#non-linearity",
    "title": "From Scratch Pt II",
    "section": "Non-Linearity",
    "text": "Non-Linearity\nThe chapter wraps up by explaining how a ReLU works, and how you can sandwich one between two linear functions to get that magical universal approximation theorem sequence of short lines to follow an arbitrarily wiggly function. We even get a peek at how to very quickly create one of these layered networks:\n\nsimple_net = nn.Sequential(\n  nn.Linear(28*28, 30),\n  nn.ReLU(),\n  nn.Linear(30, 1)\n)\n\nThis is super promising! To me this shows that libraries have progressed to the point where we genuinely don’t need to worry so much about these ‘from scratch’ details and can instead focus more on model architecture. I’m very glad to have covered the basics though, because I don’t think I could readily accept these helper functions had we not established what they are actually doing.\nNow having said that, I’m still in the dark about why you would choose any particular structure. Why does the second linear layer have 30 inputs? The text introduces the idea that deeper networks can produce more accurate results with fewer parameters than giant single layer networks, but with the caveat that they can be harder to train.\nIt’s still hand-wavey at this point. I’m eager to learn more."
  },
  {
    "objectID": "posts/post-next/index.html#some-python-library-things-ive-learned",
    "href": "posts/post-next/index.html#some-python-library-things-ive-learned",
    "title": "From Scratch Pt II",
    "section": "Some Python (& library) things I’ve learned",
    "text": "Some Python (& library) things I’ve learned\n\nBroadcasting: a PyTorch feature (syntactic sugar) that makes working with different shaped tensors much easier and performant. Rather than looping, if a higher ranked tensor is involved in a calculation with a lower ranked tensor, the lower rank tensor will be ‘virtually’ copied (not actually copied) to become the same shape as the higher rank tensor.\nPartial: bake a few parameters of a python method to return a simpler instance of that method (like when we baked a, b, c in the quadratic function so that only x was a parameter)\n* (spread): take a list in python and apply them to the input parameters of a function. Allows you to do this:\n\n\nparams = [1, 2, 3, 4]\n\ndef f(a, b, c, d): return a + b + c + d\n\nprint(f\"f(params[0], params[1], params[2], params[3]) is: {f(params[0], params[1], params[2], params[3])}\")\nprint(f\"f(*params) is: {f(*params)}\")\n\nf(params[0], params[1], params[2], params[3]) is: 10\nf(*params) is: 10\n\n\n\nPyTorch methods that end with _: they modify the object in place. e.g. bias.zero_() modifies the bias tensor, rather than returning a modified copy."
  }
]