[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am blogging about trying to learn about machine learning because they told me to."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ben’s Machine Learning Blog",
    "section": "",
    "text": "From Scratch\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nHello Machine Learning World\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nBen Kane\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello Machine Learning World",
    "section": "",
    "text": "This is the first post in a Quarto blog. Turns out it’s Quarto, not Quatro. I only discovered that a few minutes ago.\nThen again, I didn’t know what Quatro was at all half an hour ago. I was looking at fastpages, which is a way of writing blog posts on GitHub Pages and incorporating Jupyter notebook cells and output. And what is a Jupyter notebook? Well it’s like an environment where you can mix Python code and markdown text for a very interactive way of experimenting with and demonstrating machine concepts. And you can run these notebooks in the cloud in places like Colab or Kaggle. Then you can export your notebook code to regular Python and make an app.py file that creates a Gradio interface for running a machine learning model on a HuggingFace Space like this Bear Classifier I made.\nWhat the heck did I just say?\nI’ve used Python off and on for about 15 years now. It’s been a language I dabble with for a week or two to try out something like writing a Discord bot, generating web pages from a template, doing some web scraping, or messing about with machine learning/deep learning. I play for a bit, get a bit bogged down with package management and environment management, learn about some new templating language, a new meta-meta-meta install manager with some cute name like anaconda or conda or cda or whatever. Then after my experiment runs its course, I promptly drop Python and go back to whatever it is I’m actually doing.\nIt’s a very similar to the experience I have with web development, where there are myriad packages and frameworks and versions of everything, all packed with syntactic sugar and code generation ostensibly to make it easy to pick up. But the dependencies always collapse, the documentations and the tutorials always get outdated, and you’re left trying to follow the trail from breadcrumb to breadcrumb in order to get something working, usually without really understanding how it all comes together. And it’s not for lack of trying! You can dig in and see how the tools and templates and managers all work, but ultimately it just feels like layers of frameworks obfuscating the task at hand.\nOf course, when the layers of ‘simplifying’ frameworks get to be too much… you need to make a framework that eschews all the cruft and really makes things easy.\n\n\n\nStandards\n\n\n\nLearning So Far\nAnyway, here I am writing a blog to record how I’m feeling along this journey of learning machine learning (for probably the second or third time). I’ve been following the fastai course which has been great for getting things off the ground quickly. It also recommended I start blogging, so here we are.\nSo far, I’ve done two video lessons and the two accompanying chapters of the book (which take slightly different approaches to the same concepts). I’m appreciating the reinforcement thus far, because I’ve bounced off this subject in the past.\nSome big, immediate learnings: you can use a small amount of data to fine-tune an existing model, and get something useful for a standard application to just call like any other function. The idea of integrating a trained model into something like a video game is actually incredibly… doable. Sure, there are frameworks and languages and whatever other hurdles to overcome before I just drop a method into a Unity game, but it’s really not that far off.\nIn the first couple of lessons, I’ve fine-tuned an image classifer based on the resnet18 model. We’ve even dabbled with the HuggingFace community site to make a public Space, which is a site that hosts the model I trained to classify images between black bears, grizzly bears, and teddy bears. It’s not very sophisticated, but it’s a surprisingly complete end-to-end deployment of a thing that uses an image classification model. And you can just use it right now!\n\n\n\nA basic bear classifier that I made ➡ Click to try it out on huggingface.co!\n\n\nOkay, one more thing before I go: this blog post can have Python code in it, like a notebook. I should try that out, otherwise this has just been a really, really complicated Wordpress site.\n\n\nCode\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nurls = search_images('grizzly bear', max_images=1)\ndest = 'grizzly.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\nSearching for 'grizzly bear'\n\n\n\n\n\nNow to get this blog post rendered and committed to a GitHub repo and published to GitHub Pages, preferably using an automated GitHub Action……….\nThis was a lot easier in the GeoCities days."
  },
  {
    "objectID": "posts/2023-05-03 from scratch/index.html",
    "href": "posts/2023-05-03 from scratch/index.html",
    "title": "From Scratch",
    "section": "",
    "text": "Aha, bet you didn’t think I’d write a second post! Wait, that was me."
  },
  {
    "objectID": "posts/2023-05-03-from-scratch/index.html",
    "href": "posts/2023-05-03-from-scratch/index.html",
    "title": "From Scratch",
    "section": "",
    "text": "Aha, bet you didn’t think I’d write a second post! Wait, that was me.\nIt’s been just over a week since my first post, which I wrote after completing Chapter 2 of the fast.ai “Practical Deep Learning for Coders” course. The course takes a sort of “top down” approach to the topic and has you creating an image classifer right off the bat, followed by deploying another one to a production website by the end of the second chapter. It’s fast! It’s satisfying! It’s… a bit magic.\nSo Chapter 3 brings you back down to Earth and starts explaining how this all works. This is more of a traditional groundwork-laying chapter and it’s where I’ve dabbled with this stuff in the past (and quickly dropped off from). The second time around though? Stuff is starting to stick.\nThe fastai course has both videos and a ‘book’, which is really a series of Jupyter notebooks (though you can buy a print copy if you want - I imagine it’s much less effective to be honest). They sort of cover mostly the same topics but they do handle it in different ways, and this reinforcement is working well for me. It feels slow (because you’re exploring the same ground twice) but this is a pretty fundamental and meaty chapter, so I appreciate it here.\nIn fact, by the end of the chapter, simple neural networks in PyTorch, with the exception of gradient calculations, have been pretty well demystified. I’m still not comfortable with all this stuff yet but I was able to follow along quite well.\n(Alright, I just spent 15 minutes trying to get some ipywidgets to work in this blog post but although the sliders I added worked, they didn’t result in a graph getting re-plotted. Oh well, moving on.)\nSo what have I really learned so far?\n\nA Quadratic Example\nGiven some input to a function, you want to get some ouput. To start, let’s consider a quadratic function. You collect some data from the real world, and it looks like this:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom numpy.random import normal,seed,uniform\nfrom functools import partial\nfrom fastbook import *\n\nnp.random.seed(42)\n\ndef quad(a,b,c,x): return a*x**2 + b*x + c\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\n\nf = mk_quad(3, 2, 1)\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.3, 1.5)\nplt.scatter(x,y)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1c0120ba050&gt;\n\n\n\n\n\nYou’d like to have a model (a function here) that could take some input value x and output a good prediction for y. It looks like a quadratic function would work well (because we contrived this example from one), so let’s find some values for \\(ax^2 + bx + c\\).\nWe can start with random values for a, b, and c. Then we need to figure out how good they are. In this case, we’d create a function to calculate how bad they are: a mean squared error loss function. Pick some parameter values, calculate the loss. Pick some new parameters, calculate the loss again, see if it went up or down.\n\ndef mse(preds, targets):\n  return ((preds-targets)**2).mean()\n\nNext step would be to automate it. Rather than guess and check though, we can calculate the gradient for the input values. That is, how will each change in the parameter affect the resulting loss? That way, we know which way to change a parameter and, roughly, how big of an effect it will have. PyTorch can calculate these gradients for us, by tagging the input tensor as requires_grad. That means that when the tensor is used in a calculation, PyTorch will perform some internal magic and calculate the gradient for it.\n\ndef quad_mse(params):\n  f = mk_quad(*params) # \"*\" is a Python thing that means spread \n                       # the array across the function arguments here\n  return mse(f(x), y)\n\nquad_mse([1.5, 1.5, 1.5])\n\ntensor(5.8336, dtype=torch.float64)\n\n\nHere’s how we create an input tensor for our parameters, flag it to say “please calculate the gradients”, and then do so. Note that we called backward() (for back propagation) on the resulting tensor, not the input tensor. This is because we want to figure out how a change in the input parameters will affect the output. The values of the gradients, however, live on the input tensor. It does make sense, but it also feels a bit back and forth-y.\n\n# input tensor\nabc = torch.tensor([1.5,1.5,1.5], requires_grad=True)\nabc\n\ntensor([1.5000, 1.5000, 1.5000], requires_grad=True)\n\n\n\n# output tensor\nloss = quad_mse(abc)\nloss\n\ntensor(5.8336, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nThe grad_fn=&lt;MeanBackward0&gt; shows us that PyTorch can calculate gradients from this tensor. Then we call backwards() to do it, and check the resulting grad values on the input tensor:\n\nloss.backward()\nabc.grad\n\ntensor([-5.1419,  2.8472, -1.1009])\n\n\nThis tells us that increasing the value of the first parameter will decrease the loss a fair bit, decreasing the value of the second parameter will decrease the loss a little bit, and increasing the last parameter will decrease the loss a very little bit.\nSo now take a step in that direction by modifying our input parameters and calculating the loss again. Note that since modifying the input parameter is “using the tensor in a calculation”, we need to tell PyTorch that this shouldn’t be used to update the gradients on it (or things get wonky).\n\nwith torch.no_grad():\n  abc -= abc.grad*0.01\n  loss = quad_mse(abc)\n\nprint(f'loss={loss:.2f}')\nprint(abc)\n\nloss=5.49\ntensor([1.5514, 1.4715, 1.5110], requires_grad=True)\n\n\nThe loss went from 5.8336 to 5.49, and we can see each of the parameters moved in the direction to minimize the loss! Hooray. Note that we modified abc by moving in the opposite direction of the gradient (to minimize loss, not maximize it) and we also only moved by 0.01, which is called the learning rate. Moving parameters in the opposite direction of the gradient is called gradient descent.\nIf the learning rate is too low, it will take a long (too long) time to get meaningful results. If it’s too big, the modifications to parameters will be so large that you won’t converge on a useful solution but instead bounce around getting worse results.\nOkay so now you could run this a bunch of times and get closer to an ideal set of values for a, b, and c that fit the data well. Cool, now we know how to automatically find parameters to fit a function.\n\n\nBut what about problems that aren’t modeled by quadratics?\nFinding great parameters for quadratics isn’t super useful, since they don’t inherently model a lot of things. However, what if I told you there was a magical formula that could model any problem? Well more specifically, a function that could solve any computable problem to an arbitrariliy high level of accuracy?\nFor any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together. To make it closer to the wiggly function, we just have to use shorter lines. This is known as the universal approximation theorem.\nTo start, let’s actually downgrade our quadratic example to a linear equation: \\(y = mx + b\\)\nNow if you add together a bunch of linear equations, you’re just going to get a different linear equation, which is still going to be a straight line. In order to get our “short line segment” property to follow an arbitrarily wiggly line, we need to be able to have these equations affect different parts of the output. Enter the rectified linear equation:\n\ndef rectified_linear(m,b,x):\n  y = m*x + b\n  return torch.clip(y, 0.)\n\nplot_function(partial(rectified_linear, 1,1))\n\n\n\n\nA rectified linear equation is just a linear equation where any value &lt; 0 is set to 0. That’s it.\nNow, if you combine two of these together, you can see how you could start to define arbitrary shapes:\n\ndef double_relu(m1,b1,m2,b2,x):\n  return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\nplot_function(partial(double_relu, 1, 1, -2, -1))\n\n\n\n\nNow if we add together as many of these rectified linear units (aka ReLUs), we could match any arbitrarily wiggly function to whatever degree of accuracy we desire. And it works in more dimensions too (i.e. functions with more than one input).\nOkay but where do m1,b1,m2,b2, etc all come from to match the wiggly function? Well that’s the gradient descent iterative approach from earlier.\nIn a very smoothed over and blurry nutshell: this is machine learning.\nPhew this is a long post! This actually only covers the video portion of the chapter. I’ll do a recap of the book part later."
  }
]