[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am blogging about trying to learn about machine learning because they told me to."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ben’s Machine Learning Blog",
    "section": "",
    "text": "Softmax, Collab, and Listen\n\n\n\n\n\n\n\nlearning\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nKaggle’d\n\n\n\n\n\n\n\nlearning\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nLost in the (Random) Forest\n\n\n\n\n\n\n\nlearning\n\n\nrandomforest\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nFrom Scratch… uh, Again\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nGetting Started with Natural Language Processing (NLP)\n\n\n\n\n\n\n\nlearning\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nFrom Scratch Pt II\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nFrom Scratch\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nBen Kane\n\n\n\n\n\n\n  \n\n\n\n\nHello Machine Learning World\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nBen Kane\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello Machine Learning World",
    "section": "",
    "text": "This is the first post in a Quarto blog. Turns out it’s Quarto, not Quatro. I only discovered that a few minutes ago.\nThen again, I didn’t know what Quatro was at all half an hour ago. I was looking at fastpages, which is a way of writing blog posts on GitHub Pages and incorporating Jupyter notebook cells and output. And what is a Jupyter notebook? Well it’s like an environment where you can mix Python code and markdown text for a very interactive way of experimenting with and demonstrating machine concepts. And you can run these notebooks in the cloud in places like Colab or Kaggle. Then you can export your notebook code to regular Python and make an app.py file that creates a Gradio interface for running a machine learning model on a HuggingFace Space like this Bear Classifier I made.\nWhat the heck did I just say?\nI’ve used Python off and on for about 15 years now. It’s been a language I dabble with for a week or two to try out something like writing a Discord bot, generating web pages from a template, doing some web scraping, or messing about with machine learning/deep learning. I play for a bit, get a bit bogged down with package management and environment management, learn about some new templating language, a new meta-meta-meta install manager with some cute name like anaconda or conda or cda or whatever. Then after my experiment runs its course, I promptly drop Python and go back to whatever it is I’m actually doing.\nIt’s a very similar to the experience I have with web development, where there are myriad packages and frameworks and versions of everything, all packed with syntactic sugar and code generation ostensibly to make it easy to pick up. But the dependencies always collapse, the documentations and the tutorials always get outdated, and you’re left trying to follow the trail from breadcrumb to breadcrumb in order to get something working, usually without really understanding how it all comes together. And it’s not for lack of trying! You can dig in and see how the tools and templates and managers all work, but ultimately it just feels like layers of frameworks obfuscating the task at hand.\nOf course, when the layers of ‘simplifying’ frameworks get to be too much… you need to make a framework that eschews all the cruft and really makes things easy.\n\n\n\nStandards\n\n\n\nLearning So Far\nAnyway, here I am writing a blog to record how I’m feeling along this journey of learning machine learning (for probably the second or third time). I’ve been following the fastai course which has been great for getting things off the ground quickly. It also recommended I start blogging, so here we are.\nSo far, I’ve done two video lessons and the two accompanying chapters of the book (which take slightly different approaches to the same concepts). I’m appreciating the reinforcement thus far, because I’ve bounced off this subject in the past.\nSome big, immediate learnings: you can use a small amount of data to fine-tune an existing model, and get something useful for a standard application to just call like any other function. The idea of integrating a trained model into something like a video game is actually incredibly… doable. Sure, there are frameworks and languages and whatever other hurdles to overcome before I just drop a method into a Unity game, but it’s really not that far off.\nIn the first couple of lessons, I’ve fine-tuned an image classifer based on the resnet18 model. We’ve even dabbled with the HuggingFace community site to make a public Space, which is a site that hosts the model I trained to classify images between black bears, grizzly bears, and teddy bears. It’s not very sophisticated, but it’s a surprisingly complete end-to-end deployment of a thing that uses an image classification model. And you can just use it right now!\n\n\n\nA basic bear classifier that I made ➡ Click to try it out on huggingface.co!\n\n\nOkay, one more thing before I go: this blog post can have Python code in it, like a notebook. I should try that out, otherwise this has just been a really, really complicated Wordpress site.\n\n\nCode\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nurls = search_images('grizzly bear', max_images=1)\ndest = 'grizzly.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\nSearching for 'grizzly bear'\n\n\n\n\n\nNow to get this blog post rendered and committed to a GitHub repo and published to GitHub Pages, preferably using an automated GitHub Action……….\nThis was a lot easier in the GeoCities days."
  },
  {
    "objectID": "posts/2023-05-03 from scratch/index.html",
    "href": "posts/2023-05-03 from scratch/index.html",
    "title": "From Scratch",
    "section": "",
    "text": "Aha, bet you didn’t think I’d write a second post! Wait, that was me."
  },
  {
    "objectID": "posts/2023-05-03-from-scratch/index.html",
    "href": "posts/2023-05-03-from-scratch/index.html",
    "title": "From Scratch",
    "section": "",
    "text": "Aha, bet you didn’t think I’d write a second post! Wait, that was me.\nIt’s been just over a week since my first post, which I wrote after completing Chapter 2 of the fast.ai “Practical Deep Learning for Coders” course. The course takes a sort of “top down” approach to the topic and has you creating an image classifer right off the bat, followed by deploying another one to a production website by the end of the second chapter. It’s fast! It’s satisfying! It’s… a bit magic.\nSo Chapter 3 brings you back down to Earth and starts explaining how this all works. This is more of a traditional groundwork-laying chapter and it’s where I’ve dabbled with this stuff in the past (and quickly dropped off from). The second time around though? Stuff is starting to stick.\nThe fastai course has both videos and a ‘book’, which is really a series of Jupyter notebooks (though you can buy a print copy if you want - I imagine it’s much less effective to be honest). They sort of cover mostly the same topics but they do handle it in different ways, and this reinforcement is working well for me. It feels slow (because you’re exploring the same ground twice) but this is a pretty fundamental and meaty chapter, so I appreciate it here.\nIn fact, by the end of the chapter, simple neural networks in PyTorch, with the exception of gradient calculations, have been pretty well demystified. I’m still not comfortable with all this stuff yet but I was able to follow along quite well.\n(Alright, I just spent 15 minutes trying to get some ipywidgets to work in this blog post but although the sliders I added worked, they didn’t result in a graph getting re-plotted. Oh well, moving on.)\nSo what have I really learned so far?\n\nA Quadratic Example\nGiven some input to a function, you want to get some ouput. To start, let’s consider a quadratic function. You collect some data from the real world, and it looks like this:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom numpy.random import normal,seed,uniform\nfrom functools import partial\nfrom fastbook import *\n\nnp.random.seed(42)\n\ndef quad(a,b,c,x): return a*x**2 + b*x + c\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\n\nf = mk_quad(3, 2, 1)\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.3, 1.5)\nplt.scatter(x,y)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1c0120ba050&gt;\n\n\n\n\n\nYou’d like to have a model (a function here) that could take some input value x and output a good prediction for y. It looks like a quadratic function would work well (because we contrived this example from one), so let’s find some values for \\(ax^2 + bx + c\\).\nWe can start with random values for a, b, and c. Then we need to figure out how good they are. In this case, we’d create a function to calculate how bad they are: a mean squared error loss function. Pick some parameter values, calculate the loss. Pick some new parameters, calculate the loss again, see if it went up or down.\n\ndef mse(preds, targets):\n  return ((preds-targets)**2).mean()\n\nNext step would be to automate it. Rather than guess and check though, we can calculate the gradient for the input values. That is, how will each change in the parameter affect the resulting loss? That way, we know which way to change a parameter and, roughly, how big of an effect it will have. PyTorch can calculate these gradients for us, by tagging the input tensor as requires_grad. That means that when the tensor is used in a calculation, PyTorch will perform some internal magic and calculate the gradient for it.\n\ndef quad_mse(params):\n  f = mk_quad(*params) # \"*\" is a Python thing that means spread \n                       # the array across the function arguments here\n  return mse(f(x), y)\n\nquad_mse([1.5, 1.5, 1.5])\n\ntensor(5.8336, dtype=torch.float64)\n\n\nHere’s how we create an input tensor for our parameters, flag it to say “please calculate the gradients”, and then do so. Note that we called backward() (for back propagation) on the resulting tensor, not the input tensor. This is because we want to figure out how a change in the input parameters will affect the output. The values of the gradients, however, live on the input tensor. It does make sense, but it also feels a bit back and forth-y.\n\n# input tensor\nabc = torch.tensor([1.5,1.5,1.5], requires_grad=True)\nabc\n\ntensor([1.5000, 1.5000, 1.5000], requires_grad=True)\n\n\n\n# output tensor\nloss = quad_mse(abc)\nloss\n\ntensor(5.8336, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nThe grad_fn=&lt;MeanBackward0&gt; shows us that PyTorch can calculate gradients from this tensor. Then we call backwards() to do it, and check the resulting grad values on the input tensor:\n\nloss.backward()\nabc.grad\n\ntensor([-5.1419,  2.8472, -1.1009])\n\n\nThis tells us that increasing the value of the first parameter will decrease the loss a fair bit, decreasing the value of the second parameter will decrease the loss a little bit, and increasing the last parameter will decrease the loss a very little bit.\nSo now take a step in that direction by modifying our input parameters and calculating the loss again. Note that since modifying the input parameter is “using the tensor in a calculation”, we need to tell PyTorch that this shouldn’t be used to update the gradients on it (or things get wonky).\n\nwith torch.no_grad():\n  abc -= abc.grad*0.01\n  loss = quad_mse(abc)\n\nprint(f'loss={loss:.2f}')\nprint(abc)\n\nloss=5.49\ntensor([1.5514, 1.4715, 1.5110], requires_grad=True)\n\n\nThe loss went from 5.8336 to 5.49, and we can see each of the parameters moved in the direction to minimize the loss! Hooray. Note that we modified abc by moving in the opposite direction of the gradient (to minimize loss, not maximize it) and we also only moved by 0.01, which is called the learning rate. Moving parameters in the opposite direction of the gradient is called gradient descent.\nIf the learning rate is too low, it will take a long (too long) time to get meaningful results. If it’s too big, the modifications to parameters will be so large that you won’t converge on a useful solution but instead bounce around getting worse results.\nOkay so now you could run this a bunch of times and get closer to an ideal set of values for a, b, and c that fit the data well. Cool, now we know how to automatically find parameters to fit a function.\n\n\nBut what about problems that aren’t modeled by quadratics?\nFinding great parameters for quadratics isn’t super useful, since they don’t inherently model a lot of things. However, what if I told you there was a magical formula that could model any problem? Well more specifically, a function that could solve any computable problem to an arbitrariliy high level of accuracy?\nFor any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together. To make it closer to the wiggly function, we just have to use shorter lines. This is known as the universal approximation theorem.\nTo start, let’s actually downgrade our quadratic example to a linear equation: \\(y = mx + b\\)\nNow if you add together a bunch of linear equations, you’re just going to get a different linear equation, which is still going to be a straight line. In order to get our “short line segment” property to follow an arbitrarily wiggly line, we need to be able to have these equations affect different parts of the output. Enter the rectified linear equation:\n\ndef rectified_linear(m,b,x):\n  y = m*x + b\n  return torch.clip(y, 0.)\n\nplot_function(partial(rectified_linear, 1,1))\n\n\n\n\nA rectified linear equation is just a linear equation where any value &lt; 0 is set to 0. That’s it.\nNow, if you combine two of these together, you can see how you could start to define arbitrary shapes:\n\ndef double_relu(m1,b1,m2,b2,x):\n  return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\nplot_function(partial(double_relu, 1, 1, -2, -1))\n\n\n\n\nNow if we add together as many of these rectified linear units (aka ReLUs), we could match any arbitrarily wiggly function to whatever degree of accuracy we desire. And it works in more dimensions too (i.e. functions with more than one input).\nOkay but where do m1,b1,m2,b2, etc all come from to match the wiggly function? Well that’s the gradient descent iterative approach from earlier.\nIn a very smoothed over and blurry nutshell: this is machine learning.\nPhew this is a long post! This actually only covers the video portion of the chapter. I’ll do a recap of the book part later."
  },
  {
    "objectID": "posts/post-next/index.html",
    "href": "posts/post-next/index.html",
    "title": "From Scratch Pt II",
    "section": "",
    "text": "I’m back, covering the second part of chapter 3: the book part. Which is roughly chapter 4 of the book. Getting “Don’t Do What Donny Don’t Does” vibes with this every so often.\nAnyway, this chapter worked with part of the MNIST dataset, which is a bunch of small 28x28 images of hand-written digits. It’s just a partial dataset, looking at 3s and 7s, and trying to distinguish them.\nWe started by coming up with a non-machine learning baseline: stacking all the images of the 3s and averaging them to get a fuzzy “ideal” 3. Repeat for the 7s. Then for any test image, we could compare how closely it matched each of the ‘ideal’ digits. In theory, any given 3 would have more in common with the fuzzy averaged 3 than the fuzzy average 7.\nHere we learned about how to quantify things like ‘how closely an image matches another’ by using a loss function such as the L1 norm. Turns out that’s just the mean absolute difference: (test3 -avg3).abs().mean() There’s also the root mean squared error, where you square the difference, take the mean, then take the square root: ((test3 - avg3)**2).mean().sqrt()\nBoth are just ways of ensuring errors in either direction (positive and negative) are able to accumulate rather than just cancelling out.\nTurns out this approach isn’t bad! But the test case is very basic: 3s and 7s are pretty different after all."
  },
  {
    "objectID": "posts/post-next/index.html#an-end-to-end-example-of-stochastic-gradient-descent",
    "href": "posts/post-next/index.html#an-end-to-end-example-of-stochastic-gradient-descent",
    "title": "From Scratch Pt II",
    "section": "An end to end example of Stochastic Gradient Descent",
    "text": "An end to end example of Stochastic Gradient Descent\nThis was great. It was the parabola example from the video, but I implemented it all the way through for this 7 step machine learning process:\n\n\n\nThis is how machine learning works it would seem\n\n\n\nInitialize the parameters\nCalculate the predictions\nCalculate the loss\nCalculate the gradients\nStep the weights\nRepeat the process\nStop\n\n“Initialize the parameters” is the stochastic part: i.e. choose random values. “Calculate the predictions” means run the quadratic function with our current parameter values. Here we also plotted results, which was great visualization. “Calculate the loss” was done by writing our own mean squared error function (see above). “Calculate the gradients” was the loss.backward() call to get the params.grad value. “Step the weights” is the gradient descent part of the name (subtract a small fraction [learning rate] of the gradient from the params) “Repeat the process” and “Stop” were some loops.\nThis part was great! It was a solid reinforcement of the same concept shown in the video."
  },
  {
    "objectID": "posts/post-next/index.html#apply-this-to-the-mnist-loss-function",
    "href": "posts/post-next/index.html#apply-this-to-the-mnist-loss-function",
    "title": "From Scratch Pt II",
    "section": "Apply this to the MNIST Loss Function",
    "text": "Apply this to the MNIST Loss Function\nNext we did the same process but with the MNIST data set. Immediately we run into some more practical considerations regarding the function we have to use and the dimensions of the problem.\nWe know we’re going to model it with a linear equation (\\(y = mx + b\\)). But the input isn’t just a single \\(x\\), it’s a 28x28 image. So really it’s \\(y = m_1x_1 + m_2x_2 + m_3x_3 + ... + m_{784}x_{784} + b\\). I think. We end up with a tensor of size 784 to hold the weights (parameters) for our model.\nHere we start learning about performance optimizations already. Doing a repeated sum of products as described above is just matrix multiplication, which is def linear1(xb): xb@weights + bias.\nAnother important concept is the difference between our metric and the loss function. Basically, the metric is something we use to drive human understanding whereas the loss function is something we use to drive the machine learning process.\nThe difference: our metric here is accuracy, which is a binary right or wrong for every image in the test set. It could take a big change in the behaviour of the model to flip a prediction from wrong to right (or vice versa), so it wouldn’t be good for taking tiny learning steps. Conversely, the loss function conveys how close (or far) a prediction is from being correct and can be used to tune a model. It’s not as immediately intuitive for human consumption though (although arguably people may care about how ‘confident’ a prediction is).\nThere’s more work getting this example through the process, a bunch of which is bogged down somewhat by wrangling tensor dimensions. As we go through each part though, we find (unsurprisingly!) that PyTorch and/or fastai have built-in definitions for common functions like sigmoid() (for ensuring data is in the range \\((0, 1)\\)), creating linear models and initializing parameters (nn.Linear), a stochastic gradient descent optimizater (SGD(model, learning_rate)), and a learning process loop (Learner, which takes a DataLoaders object, which itself is a DataLoader for both the training and validation sets). It’s all good stuff.\nBasically we write it all from scratch to understand what something like this is actually doing:\n\nlearn = Learner(DataLoaders(dl, valid_dl), nn.Linear(28*28), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)\nlearn.fit(10, lr=1)"
  },
  {
    "objectID": "posts/post-next/index.html#non-linearity",
    "href": "posts/post-next/index.html#non-linearity",
    "title": "From Scratch Pt II",
    "section": "Non-Linearity",
    "text": "Non-Linearity\nThe chapter wraps up by explaining how a ReLU works, and how you can sandwich one between two linear functions to get that magical universal approximation theorem sequence of short lines to follow an arbitrarily wiggly function. We even get a peek at how to very quickly create one of these layered networks:\n\nsimple_net = nn.Sequential(\n  nn.Linear(28*28, 30),\n  nn.ReLU(),\n  nn.Linear(30, 1)\n)\n\nThis is super promising! To me this shows that libraries have progressed to the point where we genuinely don’t need to worry so much about these ‘from scratch’ details and can instead focus more on model architecture. I’m very glad to have covered the basics though, because I don’t think I could readily accept these helper functions had we not established what they are actually doing.\nNow having said that, I’m still in the dark about why you would choose any particular structure. Why does the second linear layer have 30 inputs? The text introduces the idea that deeper networks can produce more accurate results with fewer parameters than giant single layer networks, but with the caveat that they can be harder to train.\nIt’s still hand-wavey at this point. I’m eager to learn more."
  },
  {
    "objectID": "posts/post-next/index.html#some-python-library-things-ive-learned",
    "href": "posts/post-next/index.html#some-python-library-things-ive-learned",
    "title": "From Scratch Pt II",
    "section": "Some Python (& library) things I’ve learned",
    "text": "Some Python (& library) things I’ve learned\n\nBroadcasting: a PyTorch feature (syntactic sugar) that makes working with different shaped tensors much easier and performant. Rather than looping, if a higher ranked tensor is involved in a calculation with a lower ranked tensor, the lower rank tensor will be ‘virtually’ copied (not actually copied) to become the same shape as the higher rank tensor.\nPartial: bake a few parameters of a python method to return a simpler instance of that method (like when we baked a, b, c in the quadratic function so that only x was a parameter)\n* (spread): take a list in python and apply them to the input parameters of a function. Allows you to do this:\n\n\nparams = [1, 2, 3, 4]\n\ndef f(a, b, c, d): return a + b + c + d\n\nprint(f\"f(params[0], params[1], params[2], params[3]) is: {f(params[0], params[1], params[2], params[3])}\")\nprint(f\"f(*params) is: {f(*params)}\")\n\nf(params[0], params[1], params[2], params[3]) is: 10\nf(*params) is: 10\n\n\n\nPyTorch methods that end with _: they modify the object in place. e.g. bias.zero_() modifies the bias tensor, rather than returning a modified copy."
  },
  {
    "objectID": "posts/2023-05-10-intro-to-nlp/index.html",
    "href": "posts/2023-05-10-intro-to-nlp/index.html",
    "title": "Getting Started with Natural Language Processing (NLP)",
    "section": "",
    "text": "Dipping the tippy tip of my tippy toe into the NLP waters has been very interesting. Off the bat, figuring out how to put arbitrary text into a neural network is a great light-bulb moment.\nBasically, you split text up into tokens (roughly “pieces of words”) and then assign each token a unique number (creating a vocabulary - just a dictionary of ids and tokens). Then you can take your input text, tokenize it (split into tokens), and convert those tokens to numbers according to the vocabulary (numericalization). Now you have an array of numbers to feed into a neural network!\n\nA couple of questions\n\nWhat about tokens that aren’t in the vocabulary? Presumably these just get assigned new, unique ids next in line, and the model doesn’t have any tuned weights to do anything immediately useful with them. But since tokens are smaller than words, you probably end up covered most of the time?\nHow is “document length” really handled? Our image recognition network expected images to be 28x28 and thus expected an input of 784 numbers. Does the model just have a max size that is padded with empty tokens when a smaller document is fed in?\n\nIf so, that would explain why ChatGPT has a max document length. But why are API calls charged by the token? Wouldn’t “running the model” take the same amount of compute in this case? Maybe things are iterated… but then why a max? Need to look into how an LLM like ChatGPT actually works.\n\n\nA couple of neat learnings\nEDA is exploratory data analysis and it’s the first thing a lot of Kaggle competition entry notebooks do. Look over the data, visualize it, reason about its features, etc. Seems kinda fun! I need to learn pandas and matplotlib a lot more to be able to do this effectively.\nULMFit: this is Universal Language Model Fine-tuning, which is an approach where you take a pre-trained model that was developed on say, the entire text of Wikipedia. This is a good model for interpreting English text. Then you fine tune it with something related to your subject matter, like the contents of all the movie reviews on IMDb. This gives you a model that is more suited to the style of writing found in movie reviews (as opposed to the more formal Wikipedia), and likely includes way more movie-specific terms, like names of actors. Then you create a movie review sentiment classifier (positive or negative) using your new IMDb model. This apparently gives better results than if you just tried to make a movie review classifier directly from the Wikipedia model.\nTokenizers: You need to tokenize your input text in the same way the pre-trained model tokenized things (or else you’d be working with different vocabularies and the results would be nonsensical).\n\n\nShapes are still hard\n\nimport numpy as np, matplotlib.pyplot as plt\n\ndef f(x): return -3*x**2 + 2*x + 20\n\ndef plot_function(f, min=-2, max=2):\n  x = np.linspace(min, max, 100)\n  plt.plot(x, f(x))\n\nplot_function(f)\n\n\n\n\nThis part makes sense to me. However, we later plotted a polynomial for f that expected x to be a 2D array instead of a 1D array. It was the result of called to sklearn.pipeline.make_pipeline() to create a model, fitting it to some data, and then plotting the resulting model.predict function. The details aren’t super important, but the gist is that a 2D array for the (independent?) variable x was needed.\nTo do this, we can reshape! I was also introduced to the NumPy concept of advanced indexing/slicing where you can specify an extra axis using None to add a dimension:\n\nmin=0\nmax=3\nstep=4\nx = np.linspace(min, max, step)\nprint(f'original (1d): {x}')\n\nx = np.linspace(min, max, step).reshape(-1,1)\nprint(f'reshaped (2d):\\n{x}')\n\nx = np.linspace(min, max, step)[:,None]\nprint(f'using advanced indexing (2d, same):\\n{x}')\n\noriginal (1d): [0. 1. 2. 3.]\nreshaped (2d):\n[[0.]\n [1.]\n [2.]\n [3.]]\nusing advanced indexing (2d, same):\n[[0.]\n [1.]\n [2.]\n [3.]]\n\n\nIt’s amazing how often there will be a small thing like [:,None] thrown into the the plot_function method and I spiral off down a rabbit hole. I feel like I’m finding my way along by grabbing at the walls and fumbling in the dark, but honestly I’m happy that I do kind of figure it out in the end.\n\n\nMore consequences of the pace of development\nThe video is an example of NLP using one of the “Transformer” models on HuggingFace. It’s not clear what a “Transformer” is. The prior way of doing this was using “ULMFit”, which is apparently based on a recurrent neural network (RNN).\nTrouble is, we haven’t covered that yet. So the chapter in the book (happens to be chapter 10(!!)) isn’t something I can readily jump to (as it references chapters 8 and 9).\nWill the (video) course come back to this? Who knows!"
  },
  {
    "objectID": "next-post.html",
    "href": "next-post.html",
    "title": "From Scratch Pt II",
    "section": "",
    "text": "// This file is not in source control"
  },
  {
    "objectID": "posts/2023-05-08-from-scratch-ii/index.html",
    "href": "posts/2023-05-08-from-scratch-ii/index.html",
    "title": "From Scratch Pt II",
    "section": "",
    "text": "I’m back, covering the second part of chapter 3: the book part. Which is roughly chapter 4 of the book. Getting “Don’t Do What Donny Don’t Does” vibes with this every so often.\nAnyway, this chapter worked with part of the MNIST dataset, which is a bunch of small 28x28 images of hand-written digits. It’s just a partial dataset, looking at 3s and 7s, and trying to distinguish them.\nWe started by coming up with a non-machine learning baseline: stacking all the images of the 3s and averaging them to get a fuzzy “ideal” 3. Repeat for the 7s. Then for any test image, we could compare how closely it matched each of the ‘ideal’ digits. In theory, any given 3 would have more in common with the fuzzy averaged 3 than the fuzzy average 7.\nHere we learned about how to quantify things like ‘how closely an image matches another’ by using a loss function such as the L1 norm. Turns out that’s just the mean absolute difference: (test3 -avg3).abs().mean() There’s also the root mean squared error, where you square the difference, take the mean, then take the square root: ((test3 - avg3)**2).mean().sqrt()\nBoth are just ways of ensuring errors in either direction (positive and negative) are able to accumulate rather than just cancelling out.\nTurns out this approach isn’t bad! But the test case is very basic: 3s and 7s are pretty different after all."
  },
  {
    "objectID": "posts/2023-05-08-from-scratch-ii/index.html#an-end-to-end-example-of-stochastic-gradient-descent",
    "href": "posts/2023-05-08-from-scratch-ii/index.html#an-end-to-end-example-of-stochastic-gradient-descent",
    "title": "From Scratch Pt II",
    "section": "An end to end example of Stochastic Gradient Descent",
    "text": "An end to end example of Stochastic Gradient Descent\nThis was great. It was the parabola example from the video, but I implemented it all the way through for this 7 step machine learning process:\n\n\n\nThis is how machine learning works it would seem\n\n\n\nInitialize the parameters\nCalculate the predictions\nCalculate the loss\nCalculate the gradients\nStep the weights\nRepeat the process\nStop\n\n“Initialize the parameters” is the stochastic part: i.e. choose random values. “Calculate the predictions” means run the quadratic function with our current parameter values. Here we also plotted results, which was great visualization. “Calculate the loss” was done by writing our own mean squared error function (see above). “Calculate the gradients” was the loss.backward() call to get the params.grad value. “Step the weights” is the gradient descent part of the name (subtract a small fraction [learning rate] of the gradient from the params) “Repeat the process” and “Stop” were some loops.\nThis part was great! It was a solid reinforcement of the same concept shown in the video."
  },
  {
    "objectID": "posts/2023-05-08-from-scratch-ii/index.html#apply-this-to-the-mnist-loss-function",
    "href": "posts/2023-05-08-from-scratch-ii/index.html#apply-this-to-the-mnist-loss-function",
    "title": "From Scratch Pt II",
    "section": "Apply this to the MNIST Loss Function",
    "text": "Apply this to the MNIST Loss Function\nNext we did the same process but with the MNIST data set. Immediately we run into some more practical considerations regarding the function we have to use and the dimensions of the problem.\nWe know we’re going to model it with a linear equation (\\(y = mx + b\\)). But the input isn’t just a single \\(x\\), it’s a 28x28 image. So really it’s \\(y = m_1x_1 + m_2x_2 + m_3x_3 + ... + m_{784}x_{784} + b\\). I think. We end up with a tensor of size 784 to hold the weights (parameters) for our model.\nHere we start learning about performance optimizations already. Doing a repeated sum of products as described above is just matrix multiplication, which is def linear1(xb): xb@weights + bias.\nAnother important concept is the difference between our metric and the loss function. Basically, the metric is something we use to drive human understanding whereas the loss function is something we use to drive the machine learning process.\nThe difference: our metric here is accuracy, which is a binary right or wrong for every image in the test set. It could take a big change in the behaviour of the model to flip a prediction from wrong to right (or vice versa), so it wouldn’t be good for taking tiny learning steps. Conversely, the loss function conveys how close (or far) a prediction is from being correct and can be used to tune a model. It’s not as immediately intuitive for human consumption though (although arguably people may care about how ‘confident’ a prediction is).\nThere’s more work getting this example through the process, a bunch of which is bogged down somewhat by wrangling tensor dimensions. As we go through each part though, we find (unsurprisingly!) that PyTorch and/or fastai have built-in definitions for common functions like sigmoid() (for ensuring data is in the range \\((0, 1)\\)), creating linear models and initializing parameters (nn.Linear), a stochastic gradient descent optimizater (SGD(model, learning_rate)), and a learning process loop (Learner, which takes a DataLoaders object, which itself is a DataLoader for both the training and validation sets). It’s all good stuff.\nBasically we write it all from scratch to understand what something like this is actually doing:\n\nlearn = Learner(DataLoaders(dl, valid_dl), nn.Linear(28*28), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)\nlearn.fit(10, lr=1)"
  },
  {
    "objectID": "posts/2023-05-08-from-scratch-ii/index.html#non-linearity",
    "href": "posts/2023-05-08-from-scratch-ii/index.html#non-linearity",
    "title": "From Scratch Pt II",
    "section": "Non-Linearity",
    "text": "Non-Linearity\nThe chapter wraps up by explaining how a ReLU works, and how you can sandwich one between two linear functions to get that magical universal approximation theorem sequence of short lines to follow an arbitrarily wiggly function. We even get a peek at how to very quickly create one of these layered networks:\n\nsimple_net = nn.Sequential(\n  nn.Linear(28*28, 30),\n  nn.ReLU(),\n  nn.Linear(30, 1)\n)\n\nThis is super promising! To me this shows that libraries have progressed to the point where we genuinely don’t need to worry so much about these ‘from scratch’ details and can instead focus more on model architecture. I’m very glad to have covered the basics though, because I don’t think I could readily accept these helper functions had we not established what they are actually doing.\nNow having said that, I’m still in the dark about why you would choose any particular structure. Why does the second linear layer have 30 inputs? The text introduces the idea that deeper networks can produce more accurate results with fewer parameters than giant single layer networks, but with the caveat that they can be harder to train.\nIt’s still hand-wavey at this point. I’m eager to learn more."
  },
  {
    "objectID": "posts/2023-05-08-from-scratch-ii/index.html#some-python-library-things-ive-learned",
    "href": "posts/2023-05-08-from-scratch-ii/index.html#some-python-library-things-ive-learned",
    "title": "From Scratch Pt II",
    "section": "Some Python (& library) things I’ve learned",
    "text": "Some Python (& library) things I’ve learned\n\nBroadcasting: a PyTorch feature (syntactic sugar) that makes working with different shaped tensors much easier and performant. Rather than looping, if a higher ranked tensor is involved in a calculation with a lower ranked tensor, the lower rank tensor will be ‘virtually’ copied (not actually copied) to become the same shape as the higher rank tensor.\nPartial: bake a few parameters of a python method to return a simpler instance of that method (like when we baked a, b, c in the quadratic function so that only x was a parameter)\n* (spread): take a list in python and apply them to the input parameters of a function. Allows you to do this:\n\n\nparams = [1, 2, 3, 4]\n\ndef f(a, b, c, d): return a + b + c + d\n\nprint(f\"f(params[0], params[1], params[2], params[3]) is: {f(params[0], params[1], params[2], params[3])}\")\nprint(f\"f(*params) is: {f(*params)}\")\n\nf(params[0], params[1], params[2], params[3]) is: 10\nf(*params) is: 10\n\n\n\nPyTorch methods that end with _: they modify the object in place. e.g. bias.zero_() modifies the bias tensor, rather than returning a modified copy."
  },
  {
    "objectID": "posts/2023-05-15-From-Scratch-Again/index.html",
    "href": "posts/2023-05-15-From-Scratch-Again/index.html",
    "title": "From Scratch… uh, Again",
    "section": "",
    "text": "Okay now I know we’ve done this before: making a neural network from scratch. This course is a wacky mess sometimes. It’s a new 2022 edition of an “old” (2019?) course that’s partially based on a book. It’s clumsy and that really shows with this lesson.\nHaving said that, the reinforcement of learning to make a model from scratch means it’s sinking in better each time. I’m still making mistakes when I try to write this stuff out myself first so there’s definitely still value in the repetition.\n\nNot All Repetition\nThere’s new stuff too! More practice with Pandas. More rubber-hits-the-road details dealing with handling missing data. The short answer? Don’t throw it out! You can’t keep that NaN value in there, but you can easily fill it in with the mode of that column. What’s more, a framework like fastai will automatically create an extra bool column to indicate if the related column was filled in or not. The fact that data was originally missing could itself be a useful feature!\nWe also saw how to handle continuous data with long-tail distributions (take the log!) as well as how to turn non-numerical data into a number (dummy variable categorization!). A neat thing to note is when data is stored as a number, but doesn’t actually represent a continuous range (like the Class=1,2, or 3 in Titanic passengers). Lastly, we saw the importance of normalizing data with big ranges to allow for easier optimization.\n\n\nAnd the rest\nThen there was a lot of work creating the neural network, training it, creating a network with hidden layers, and so forth. It was useful. I will need to do more of this to actually practice this stuff.\n\n\nWhat’s Tripping Me Up Today\nThis blog is more of a journal of what I’m currently struggling to understand. If anything is helpful looking back, it may be this. Plus it’s kinda helpful just to get it written down, because maybe some of these things I actually have answers for now that the lesson is finished.\n\nOddity: when we did this example in Excel, we only did Pclass_0 and Pclass_1, with Pclass = 2 being implied if the first two are false. We can do this with pd.get_dummies too, with a drop_first=True flag. However, if we do this, then we need to include a constant term in our linear equations too. If we fully specify the dummy variables, we don’t need this constant. I don’t totally understand why this is.\nIn the Titanic example, we notice that the first column, ‘Age’, has a much bigger magnitude range than the other columns. I’m told this will make it “hard to optimize.” (which we can solve by normalizing the values) But why? shouldn’t gradient descent handle this gracefully? sure it will be off at first, but eventually the weight of that column will just be smaller relatively speaking, no?\nIf we use a sigmoid function to smush predictions to [0,1] after the fact, then the optimizer doesn’t need to work really hard to get things to exactly 1 or 0 - it can be content with this bigger or smaller values. This took me a little bit, but I think this means we can have a higher learning rate and still have it be stable. The optimizer doesn’t need to try to nail a ‘1’ to indicate “survived”. It can generate a 1.4, which is smushed to 1, and have a loss of zero. Without it, a prediction of 1.4 and another of 0.6 would both have equal loss values (I think).\nWhat about categories that only appear in test or real world data? (for dummy variables) For this, I think we need an “Other” category. Frameworks like fastai might already do this?\nTensor shapes. Still. We made a deep neural net with two hidden layers, so that meant expanding the coefficients to be a matrix instead of a vector. Every time, I was able to work my way to an understanding, but it still isn’t intuitive yet. The idea of “number of outputs of layer n must match the number of inputs at layer n + 1” is fine, but whenever I start thinking about the matrix multiplication and what these matrices represent, I get bogged down quickly.\nWhen manually creating some of these neural network examples, there were things like -0.3 and /n_hidden which Jeremy described as “fiddly” to find values to “get it to train”. I’m not sure what this means or what something training vs not training looks like.\n\n\n\nBonus Tidbits\n\nWe expanded our predictions vector by one dimension to have a shape like [713, 1]. This is called a rank 2 tensor (matrix) with a “trailing unit axis”.\nNOTEBOOK TIP: there is a shortcut to split a code cell into a new cell by line. This effectively allows you to kinda of ‘debug’ and go line by line to see how it all works.\nInteresting result: the linear model, the two layer model, and the deep model all resulted in the same effective accuracy on the limited Titanic data set."
  },
  {
    "objectID": "posts/2023-06-02-lost-in-the-forest/index.html",
    "href": "posts/2023-06-02-lost-in-the-forest/index.html",
    "title": "Lost in the (Random) Forest",
    "section": "",
    "text": "Uh oh, it has been a while. The challenge of learning something new on a self-directed schedule is always when the momentum falters and the shiny fun newness has worn off. In the first lesson or two, we were making cool toys very quickly, without getting bogged down in specifics or inner workings.\nWell we’re in the inner workings now.\nI haven’t had much time to work on the course for a couple of weeks and the risk of just stopping altogether is pretty high. Feeling like I “have” to write these blog posts rather than moving forward with the content honestly doesn’t help, but I do believe that writing them (as well as practicing the notebooks) is critical. If I just watch the videos, nothing will stick at all.\n\nVideo 6: Decision Trees and Random Forests\nThis chapter was an interesting one, where we returned to tabular data and tried out an approach that doesn’t use machine learning at all. We created binary splits on our data by picking a column and then a split threshold for the values in that column, then seeing how the resulting sets looked. Like if we split based on the “male”/“female” column in the Titanic data set, how good a predictor of the dependent “Survived” variable is that? If we further split those groups based on another variable, how good a predictor is the model now? This is called a decision tree.\nThen we get to the mind-blowing idea of random forests. Here, we make a lot of decision trees that are made using only a subset of the available data (and even the available columns). Then we average their predictions. Somehow, the average of these predictions will cancel out all of the uncorrelated errors present in each tree and give us a more accurate result.\nThere ends up being a fairly harsh limitation to this approach on a data set as small as the Titanic competition, but it’s still fascinating. It also makes for a super easy baseline to start iterating from.\nOne super sweet thing is you can use tools from the sklearn framework to easily find out feature importance, which is a measure of how effective each property of a dataset is when creating these splits. In other words, you can quickly see what the most important columns in your data are!\n\n\n\nFeature importance is cool\n\n\n\n\nKaggle\nThe second part of the video focused on a practical run through a Kaggle competition, and how to iterate very quickly. I think I need to actually try a Kaggle competition to see how well I can remember any of this stuff and how much I can understand how to apply it without guidance.\nI suspect I wouldn’t know where to start.\n\n\nOnward\nPerfect is the enemy of good. I haven’t mastered the material yet, but I want to move on because losing motivation/momentum is more dangerous than not fully grasping things."
  },
  {
    "objectID": "posts/2023-06-07-Kaggled/index.html",
    "href": "posts/2023-06-07-Kaggled/index.html",
    "title": "Kaggle’d",
    "section": "",
    "text": "Previously, on BLOG:\n\nI think I need to actually try a Kaggle competition to see how well I can remember any of this stuff and how much I can understand how to apply it without guidance.\n\nWell, I’ve spent a couple days iterating on the Spaceship Titanic competition on Kaggle, which is another starter competition like the original Titanic competition. This one is totally fabricated, but it has a similar structure: predict who got ‘transported’ to an alternate dimension!\n\nBaby Steps and Basic Recall\nJeremy’s advice is to submit as soon as possible, and then iterate quickly. Sounds like good software development to me! I made a notebook and then followed the lesson notes for creating a very basic decision tree. Mostly this was an exercise in practicing manipulating data frames and getting familiar with the data columns. I used the sklearn DecisionTreeClassifier (rather than creating it from scratch), ran the test data set through the predictor, packaged up the .csv, hit “Submit” and…\nScored 0.\nOkay, double-checked the expected data format and changed 1/0 to True/False, submitted again, and…\n\n\n\nScored 0.7255 with a single Decision Tree!\n\n\n\n\nIterating: RandomForests and Feature Engineering\nThe next step was to try out a RandomForest, which was really just a simple change to use sklearn.ensemble.RandomForestClassifier instead of the decision tree. Iterating quickly meant it was time to submit again immediately! This instantly garnered a big improvement to about 0.79. For context, the top of the leaderboard is scoring about 0.81.\nBut what I wanted to do next was some “Feature Engineering”. To me, at this stage, this meant making new columns out of existing columns. There was some low hanging fruit, because the “Cabin” was described as having a format of “Deck/CabinNumber/SideOfShip”, so this string could be parsed into more useful categories.\nThis gave me some practice working with pandas, but ultimately didn’t move the needle much. The variation in how the forest was created along with the various columns I was adding actually resulted in slightly worse scores most of the time. I did discover that creating a “Total Spend” column by summing the individual expenditure columns resulted in a useful feature, but that was really just luck.\n\n\n\nFeature Importance\n\n\n\n\nMachine Learning\nNext up, it was time to actually do machine learning! Then I remembered I’d have to deal with normalizing values, filling in missing data, creating dummy variables… argh! Then I remembered that fastai is supposed to handle this for me, so I got to work.\nThe hardest part here was just remembering how to actually put this stuff together, and I used one of Jeremy’s notebooks as a reference for making a TabularPandas DataLoaders object and then using tabular_learner to train it.\nI even ensembled 5 of these models together, taking their average predictions.\nThis performed almost as poorly as my initial decision tree. Clearly there’s more to this. However, I was proud to have gotten to this point because although I needed references from other notebooks, I was applying what I had learned to a new problem.\n\n\nWalkthrough and Better Feature Engineering\nAfter I had done this, I wanted to look up “how to do it properly”. I found a great notebook walkthrough called (Spaceship Titanic Competition End to End Project)[https://www.kaggle.com/code/kdsharma/spaceship-titanic-competition-end-to-end-project] and ended up following it through.\nThis was super useful because I got a lot of practice visualizing pieces of the data set and actually reasoning about what I was seeing. Things with really high cardinality aren’t especially useful for making predictions. Sometimes continuous variables can be grouped for to help make better categories. Lots of great stuff.\nThe process of doing “Exploratory Data Analysis” followed by “Feature Engineering” followed by experimentations in training approaches feels really fun! It’s like unraveling a puzzle slowly.\nI ended up mostly typing out the feature engineering of the walkthrough myself (which was great practice and has me feeling more comfortable with matplotlib and seaborn every time). The final submission I did resulted in this:\n\n\n\nTop 1000 baby\n\n\n\n\nToday’s Concerns\nTowards the end of the walkthrough, it gets to the models and training, which is all way over my head. I don’t know about these yet and this wasn’t the avenue to learn them. Instead, I just used fastai and the tabular_learner. Even here though, I was just playing around with learning rate, layer sizes, and number of epochs/ensembles.\nIt feels gross.\nThe exploratory data analysis felt kind of hand-wavey, but you could see where more rigor could be applied. This part though? It feels like guessing and checking amongst a plethora of rules of thumb. I think this is the hyperparameter tuning part of things, and I believe there are automation tools to help with it. For now though, it feels like the difference between 0.79 and 0.81 is fiddly and unscientific.\nHopefully that’s just because I don’t know enough yet. I’d hate for the science in data science to be a fraud."
  },
  {
    "objectID": "posts/2023-06-19-collab/index.html",
    "href": "posts/2023-06-19-collab/index.html",
    "title": "Softmax, Collab, and Listen",
    "section": "",
    "text": "Video 7 is now complete. It’s a bit of grab bag but here are some of the coolest bits that stuck with me:"
  },
  {
    "objectID": "posts/2023-06-19-collab/index.html#gradient-accumulation",
    "href": "posts/2023-06-19-collab/index.html#gradient-accumulation",
    "title": "Softmax, Collab, and Listen",
    "section": "Gradient Accumulation",
    "text": "Gradient Accumulation\nAlso known as “don’t do the gradient step immediately after you feed a batch through the GPU”. Instead, feed in part of a batch, then add the effect of that partial batch to the gradients (i.e. accumulate). Then repeat until you have completed all pieces of your batch, then do the gradient step as normal.\nWhy? Because you can fit effectively an infinite batch size into GPU memory this way. There do seem to be some caveats with some model types, but for the stuff we’ve been doing, it should be mathematically equivalent."
  },
  {
    "objectID": "posts/2023-06-19-collab/index.html#predicting-two-things",
    "href": "posts/2023-06-19-collab/index.html#predicting-two-things",
    "title": "Softmax, Collab, and Listen",
    "section": "Predicting Two Things",
    "text": "Predicting Two Things\nWe did the ‘rice paddy’ example but this time tried to predict both the type of rice variety as well as the disease. A “mind blown” moment was when Jeremy said we’re not actually going from one prediction to two: we’re going from 10 (diseases) to 20 (diseases + varieties).\nThere we some other neat learnings in pulling this off too. We needed to examine the loss function more closely. Before, we had let fastai give us a default function of ‘cross entropy’. We just kind of ignored that something like mean squared error wouldn’t work when there are 10 labels to predict (not one)."
  },
  {
    "objectID": "posts/2023-06-19-collab/index.html#softmax-and-cross-entropy",
    "href": "posts/2023-06-19-collab/index.html#softmax-and-cross-entropy",
    "title": "Softmax, Collab, and Listen",
    "section": "Softmax and Cross-Entropy",
    "text": "Softmax and Cross-Entropy\nThese seemed confusing at first, but ultimately ended up being “raise e to the power of the output value”, then “divide by the sum of all of these”. These values can then be treated as probabilities, with the added bonus that the process tends to push one prediction to the top.\n\n\n\nSoftmax!\n\n\nCross-entropy then ends up being the case where you just take the (negative) softmax of the prediction for the label that was actually correct. But what’s neat is that this “look up the prediction for the correct label” gets expressed as “add up a one-hot vector times all the predictions”.\n\n\n\nDaunting, but actually just “look up the prediction for the correct label”"
  },
  {
    "objectID": "posts/2023-06-19-collab/index.html#collaborative-filtering-and-latent-factors",
    "href": "posts/2023-06-19-collab/index.html#collaborative-filtering-and-latent-factors",
    "title": "Softmax, Collab, and Listen",
    "section": "Collaborative Filtering and Latent Factors",
    "text": "Collaborative Filtering and Latent Factors\nAnother surprising thing was the idea of collaborative filtering on very basic datasets. We looked at movie reviews and posited that it would be useful to be able to compare features of a movie to a user’s preferences. The problem is we don’t have any of that data.\nWhat’s neat is that it doesn’t matter! We can look at patterns of users and the ratings for movies that we do have and just… pretend that such features exist. We call ’em “latent factors”, and we can do some fitting to get values for those latent factors that work.\nWeird bit: what does those factors actually represent? Probably nothing meaningful in the real world. I imagine you can’t map one of them to genre or release date for instance."
  },
  {
    "objectID": "posts/2023-06-19-collab/index.html#embeddings",
    "href": "posts/2023-06-19-collab/index.html#embeddings",
    "title": "Softmax, Collab, and Listen",
    "section": "Embeddings",
    "text": "Embeddings\nEmbeddings are another ‘fancy term for something basic’, as it turns out to just mean “look something up in an array”. In this case, we made an embedding for the user latent factors and another for the movie latent factors. We ended up with a table of users where the extra columns were the latent factors, and similarly for the movies. To make a prediction of a particular user and a particular movie, we needed to ‘look up’ the latent factors for that user and take the dot product of them with the latent factors of that particular movie (which also needed to be looked up)."
  }
]