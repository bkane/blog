{
  "hash": "5ff750abdefe4fa3246d9e307a84f0c4",
  "result": {
    "markdown": "---\ntitle: \"From Scratch\"\nauthor: \"Ben Kane\"\ndate: \"2023-05-03\"\ncategories: [learning]\nformat:\n  html:\n    code-fold: false\n---\n\nAha, bet you didn't think I'd write a second post! Wait, that was me.\n\nIt's been just over a week since my first post, which I wrote after completing Chapter 2 of the [fast.ai \"Practical Deep Learning for Coders\" course](https://course.fast.ai). The course takes a sort of \"top down\" approach to the topic and has you creating an image classifer right off the bat, followed by deploying another one to a production website by the end of the second chapter. It's fast! It's satisfying! It's... a bit magic.\n\nSo Chapter 3 brings you back down to Earth and starts explaining how this all works. This is more of a traditional groundwork-laying chapter and it's where I've dabbled with this stuff in the past (and quickly dropped off from). The second time around though? Stuff is starting to stick.\n\nThe fastai course has both videos and a 'book', which is really a series of Jupyter notebooks (though you can buy a print copy if you want - I imagine it's much less effective to be honest). They sort of cover mostly the same topics but they do handle it in different ways, and this reinforcement is working well for me. It feels slow (because you're exploring the same ground twice) but this is a pretty fundamental and meaty chapter, so I appreciate it here.\n\nIn fact, by the end of the chapter, simple neural networks in PyTorch, with the exception of gradient calculations, have been pretty well demystified. I'm still not comfortable with all this stuff yet but I was able to follow along quite well.\n\n(Alright, I just spent 15 minutes trying to get some `ipywidgets` to work in this blog post but although the sliders I added worked, they didn't result in a graph getting re-plotted. Oh well, moving on.)\n\nSo what have I really learned so far?\n\n# A Quadratic Example\nGiven some input to a function, you want to get some ouput. To start, let's consider a quadratic function. You collect some data from the real world, and it looks like this:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom numpy.random import normal,seed,uniform\nfrom functools import partial\nfrom fastbook import *\n\nnp.random.seed(42)\n\ndef quad(a,b,c,x): return a*x**2 + b*x + c\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\n\nf = mk_quad(3, 2, 1)\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.3, 1.5)\nplt.scatter(x,y)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n<matplotlib.collections.PathCollection at 0x1c0120ba050>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=570 height=414}\n:::\n:::\n\n\nYou'd like to have a model (a function here) that could take some input value x and output a good prediction for y. It looks like a quadratic function would work well (because we contrived this example from one), so let's find some values for $ax^2 + bx + c$.\n\nWe can start with random values for a, b, and c. Then we need to figure out how *good* they are. In this case, we'd create a function to calculate how bad they are: a *mean squared error* loss function. Pick some parameter values, calculate the loss. Pick some new parameters, calculate the loss again, see if it went up or down.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef mse(preds, targets):\n  return ((preds-targets)**2).mean()\n```\n:::\n\n\nNext step would be to automate it. Rather than guess and check though, we can calculate the *gradient* for the input values. That is, how will each change in the parameter affect the resulting loss? That way, we know which way to change a parameter and, roughly, how big of an effect it will have. PyTorch can calculate these gradients for us, by tagging *the input tensor* as `requires_grad`. That means that when the tensor is used in a calculation, PyTorch will perform some internal magic and calculate the gradient for it.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef quad_mse(params):\n  f = mk_quad(*params) # \"*\" is a Python thing that means spread \n                       # the array across the function arguments here\n  return mse(f(x), y)\n\nquad_mse([1.5, 1.5, 1.5])\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\ntensor(5.8336, dtype=torch.float64)\n```\n:::\n:::\n\n\nHere's how we create an input tensor for our parameters, flag it to say \"please calculate the gradients\", and then do so. Note that we called `backward()` (for *back propagation*) on the *resulting* tensor, not the input tensor. This is because we want to figure out how a change in the input parameters will affect the output. The values of the gradients, however, live on the input tensor. It does make sense, but it also feels a bit back and forth-y.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# input tensor\nabc = torch.tensor([1.5,1.5,1.5], requires_grad=True)\nabc\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\ntensor([1.5000, 1.5000, 1.5000], requires_grad=True)\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# output tensor\nloss = quad_mse(abc)\nloss\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\ntensor(5.8336, dtype=torch.float64, grad_fn=<MeanBackward0>)\n```\n:::\n:::\n\n\nThe `grad_fn=<MeanBackward0>` shows us that PyTorch can calculate gradients from this tensor. Then we call `backwards()` to do it, and check the resulting `grad` values on the input tensor:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nloss.backward()\nabc.grad\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\ntensor([-5.1419,  2.8472, -1.1009])\n```\n:::\n:::\n\n\nThis tells us that increasing the value of the first parameter will decrease the loss a fair bit, decreasing the value of the second parameter will decrease the loss a little bit, and increasing the last parameter will decrease the loss a very little bit.\n\nSo now take a step in that direction by modifying our input parameters and calculating the loss again. Note that since modifying the input parameter is \"using the tensor in a calculation\", we need to tell PyTorch that this *shouldn't* be used to update the gradients on it (or things get wonky).\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nwith torch.no_grad():\n  abc -= abc.grad*0.01\n  loss = quad_mse(abc)\n\nprint(f'loss={loss:.2f}')\nprint(abc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloss=5.49\ntensor([1.5514, 1.4715, 1.5110], requires_grad=True)\n```\n:::\n:::\n\n\nThe loss went from 5.8336 to 5.49, and we can see each of the parameters moved in the direction to minimize the loss! Hooray. Note that we modified `abc` by moving in the opposite direction of the gradient (to *minimize* loss, not maximize it) and we also only moved by `0.01`, which is called the *learning rate*. Moving parameters in the opposite direction of the gradient is called *gradient descent*.\n\nIf the learning rate is too low, it will take a long (too long) time to get meaningful results. If it's too big, the modifications to parameters will be so large that you won't converge on a useful solution but instead bounce around getting worse results.\n\nOkay so now you could run this a bunch of times and get closer to an ideal set of values for a, b, and c that fit the data well. Cool, now we know how to automatically find parameters to fit a function.\n\n# But what about problems that aren't modeled by quadratics?\n\nFinding great parameters for quadratics isn't super useful, since they don't inherently model a lot of things. However, what if I told you there was a magical formula that could model any problem? Well more specifically, a function that could solve any computable problem to an arbitrariliy high level of accuracy?\n\nFor any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together. To make it closer to the wiggly function, we just have to use shorter lines. This is known as the *universal approximation theorem*.\n\nTo start, let's actually downgrade our quadratic example to a linear equation: $y = mx + b$\n\nNow if you add together a bunch of linear equations, you're just going to get a different linear equation, which is still going to be a straight line. In order to get our \"short line segment\" property to follow an arbitrarily wiggly line, we need to be able to have these equations affect different parts of the output. Enter the *rectified linear equation*:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef rectified_linear(m,b,x):\n  y = m*x + b\n  return torch.clip(y, 0.)\n\nplot_function(partial(rectified_linear, 1,1))\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=500 height=340}\n:::\n:::\n\n\nA rectified linear equation is just a linear equation where any value < 0 is set to 0. That's it.\n\nNow, if you combine two of these together, you can see how you could start to define arbitrary shapes:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef double_relu(m1,b1,m2,b2,x):\n  return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\nplot_function(partial(double_relu, 1, 1, -2, -1))\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=500 height=340}\n:::\n:::\n\n\nNow if we add together as many of these *rectified linear units* (aka *ReLUs*), we could match any arbitrarily wiggly function to whatever degree of accuracy we desire. And it works in more dimensions too (i.e. functions with more than one input).\n\nOkay but where do m1,b1,m2,b2, etc all come from to match the wiggly function? Well that's the gradient descent iterative approach from earlier.\n\nIn a very smoothed over and blurry nutshell: this is machine learning.\n\nPhew this is a long post! This actually only covers the video portion of the chapter. I'll do a recap of the book part later.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}